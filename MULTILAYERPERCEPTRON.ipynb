{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d9f02e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST PARAMS: {'classifier__activation': 'tanh', 'classifier__hidden_layer_sizes': (100,), 'classifier__learning_rate': 'constant', 'classifier__learning_rate_init': 0.01, 'classifier__max_iter': 7000, 'classifier__solver': 'adam'}\n",
      "\n",
      "0.792 (+/- 0.13) for {'classifier__activation': 'relu', 'classifier__hidden_layer_sizes': (10,), 'classifier__learning_rate': 'constant', 'classifier__learning_rate_init': 0.01, 'classifier__max_iter': 7000, 'classifier__solver': 'adam'}\n",
      "0.158 (+/- 0.012) for {'classifier__activation': 'relu', 'classifier__hidden_layer_sizes': (10,), 'classifier__learning_rate': 'constant', 'classifier__learning_rate_init': 0.5, 'classifier__max_iter': 7000, 'classifier__solver': 'adam'}\n",
      "0.452 (+/- 0.47) for {'classifier__activation': 'relu', 'classifier__hidden_layer_sizes': (10,), 'classifier__learning_rate': 'constant', 'classifier__learning_rate_init': 0.1, 'classifier__max_iter': 7000, 'classifier__solver': 'adam'}\n",
      "0.164 (+/- 0.01) for {'classifier__activation': 'relu', 'classifier__hidden_layer_sizes': (10,), 'classifier__learning_rate': 'constant', 'classifier__learning_rate_init': 0.2, 'classifier__max_iter': 7000, 'classifier__solver': 'adam'}\n",
      "0.562 (+/- 0.65) for {'classifier__activation': 'relu', 'classifier__hidden_layer_sizes': (50, 10), 'classifier__learning_rate': 'constant', 'classifier__learning_rate_init': 0.01, 'classifier__max_iter': 7000, 'classifier__solver': 'adam'}\n",
      "0.161 (+/- 0.023) for {'classifier__activation': 'relu', 'classifier__hidden_layer_sizes': (50, 10), 'classifier__learning_rate': 'constant', 'classifier__learning_rate_init': 0.5, 'classifier__max_iter': 7000, 'classifier__solver': 'adam'}\n",
      "0.164 (+/- 0.01) for {'classifier__activation': 'relu', 'classifier__hidden_layer_sizes': (50, 10), 'classifier__learning_rate': 'constant', 'classifier__learning_rate_init': 0.1, 'classifier__max_iter': 7000, 'classifier__solver': 'adam'}\n",
      "0.166 (+/- 0.002) for {'classifier__activation': 'relu', 'classifier__hidden_layer_sizes': (50, 10), 'classifier__learning_rate': 'constant', 'classifier__learning_rate_init': 0.2, 'classifier__max_iter': 7000, 'classifier__solver': 'adam'}\n",
      "0.83 (+/- 0.155) for {'classifier__activation': 'relu', 'classifier__hidden_layer_sizes': (100,), 'classifier__learning_rate': 'constant', 'classifier__learning_rate_init': 0.01, 'classifier__max_iter': 7000, 'classifier__solver': 'adam'}\n",
      "0.163 (+/- 0.012) for {'classifier__activation': 'relu', 'classifier__hidden_layer_sizes': (100,), 'classifier__learning_rate': 'constant', 'classifier__learning_rate_init': 0.5, 'classifier__max_iter': 7000, 'classifier__solver': 'adam'}\n",
      "0.166 (+/- 0.002) for {'classifier__activation': 'relu', 'classifier__hidden_layer_sizes': (100,), 'classifier__learning_rate': 'constant', 'classifier__learning_rate_init': 0.1, 'classifier__max_iter': 7000, 'classifier__solver': 'adam'}\n",
      "0.166 (+/- 0.002) for {'classifier__activation': 'relu', 'classifier__hidden_layer_sizes': (100,), 'classifier__learning_rate': 'constant', 'classifier__learning_rate_init': 0.2, 'classifier__max_iter': 7000, 'classifier__solver': 'adam'}\n",
      "0.784 (+/- 0.163) for {'classifier__activation': 'tanh', 'classifier__hidden_layer_sizes': (10,), 'classifier__learning_rate': 'constant', 'classifier__learning_rate_init': 0.01, 'classifier__max_iter': 7000, 'classifier__solver': 'adam'}\n",
      "0.144 (+/- 0.022) for {'classifier__activation': 'tanh', 'classifier__hidden_layer_sizes': (10,), 'classifier__learning_rate': 'constant', 'classifier__learning_rate_init': 0.5, 'classifier__max_iter': 7000, 'classifier__solver': 'adam'}\n",
      "0.175 (+/- 0.121) for {'classifier__activation': 'tanh', 'classifier__hidden_layer_sizes': (10,), 'classifier__learning_rate': 'constant', 'classifier__learning_rate_init': 0.1, 'classifier__max_iter': 7000, 'classifier__solver': 'adam'}\n",
      "0.153 (+/- 0.025) for {'classifier__activation': 'tanh', 'classifier__hidden_layer_sizes': (10,), 'classifier__learning_rate': 'constant', 'classifier__learning_rate_init': 0.2, 'classifier__max_iter': 7000, 'classifier__solver': 'adam'}\n",
      "0.819 (+/- 0.162) for {'classifier__activation': 'tanh', 'classifier__hidden_layer_sizes': (50, 10), 'classifier__learning_rate': 'constant', 'classifier__learning_rate_init': 0.01, 'classifier__max_iter': 7000, 'classifier__solver': 'adam'}\n",
      "0.15 (+/- 0.02) for {'classifier__activation': 'tanh', 'classifier__hidden_layer_sizes': (50, 10), 'classifier__learning_rate': 'constant', 'classifier__learning_rate_init': 0.5, 'classifier__max_iter': 7000, 'classifier__solver': 'adam'}\n",
      "0.184 (+/- 0.114) for {'classifier__activation': 'tanh', 'classifier__hidden_layer_sizes': (50, 10), 'classifier__learning_rate': 'constant', 'classifier__learning_rate_init': 0.1, 'classifier__max_iter': 7000, 'classifier__solver': 'adam'}\n",
      "0.147 (+/- 0.031) for {'classifier__activation': 'tanh', 'classifier__hidden_layer_sizes': (50, 10), 'classifier__learning_rate': 'constant', 'classifier__learning_rate_init': 0.2, 'classifier__max_iter': 7000, 'classifier__solver': 'adam'}\n",
      "0.847 (+/- 0.167) for {'classifier__activation': 'tanh', 'classifier__hidden_layer_sizes': (100,), 'classifier__learning_rate': 'constant', 'classifier__learning_rate_init': 0.01, 'classifier__max_iter': 7000, 'classifier__solver': 'adam'}\n",
      "0.145 (+/- 0.025) for {'classifier__activation': 'tanh', 'classifier__hidden_layer_sizes': (100,), 'classifier__learning_rate': 'constant', 'classifier__learning_rate_init': 0.5, 'classifier__max_iter': 7000, 'classifier__solver': 'adam'}\n",
      "0.177 (+/- 0.152) for {'classifier__activation': 'tanh', 'classifier__hidden_layer_sizes': (100,), 'classifier__learning_rate': 'constant', 'classifier__learning_rate_init': 0.1, 'classifier__max_iter': 7000, 'classifier__solver': 'adam'}\n",
      "0.144 (+/- 0.022) for {'classifier__activation': 'tanh', 'classifier__hidden_layer_sizes': (100,), 'classifier__learning_rate': 'constant', 'classifier__learning_rate_init': 0.2, 'classifier__max_iter': 7000, 'classifier__solver': 'adam'}\n",
      "0.815 (+/- 0.147) for {'classifier__activation': 'logistic', 'classifier__hidden_layer_sizes': (10,), 'classifier__learning_rate': 'constant', 'classifier__learning_rate_init': 0.01, 'classifier__max_iter': 7000, 'classifier__solver': 'adam'}\n",
      "0.141 (+/- 0.025) for {'classifier__activation': 'logistic', 'classifier__hidden_layer_sizes': (10,), 'classifier__learning_rate': 'constant', 'classifier__learning_rate_init': 0.5, 'classifier__max_iter': 7000, 'classifier__solver': 'adam'}\n",
      "0.413 (+/- 0.461) for {'classifier__activation': 'logistic', 'classifier__hidden_layer_sizes': (10,), 'classifier__learning_rate': 'constant', 'classifier__learning_rate_init': 0.1, 'classifier__max_iter': 7000, 'classifier__solver': 'adam'}\n",
      "0.179 (+/- 0.161) for {'classifier__activation': 'logistic', 'classifier__hidden_layer_sizes': (10,), 'classifier__learning_rate': 'constant', 'classifier__learning_rate_init': 0.2, 'classifier__max_iter': 7000, 'classifier__solver': 'adam'}\n",
      "0.827 (+/- 0.177) for {'classifier__activation': 'logistic', 'classifier__hidden_layer_sizes': (50, 10), 'classifier__learning_rate': 'constant', 'classifier__learning_rate_init': 0.01, 'classifier__max_iter': 7000, 'classifier__solver': 'adam'}\n",
      "0.146 (+/- 0.025) for {'classifier__activation': 'logistic', 'classifier__hidden_layer_sizes': (50, 10), 'classifier__learning_rate': 'constant', 'classifier__learning_rate_init': 0.5, 'classifier__max_iter': 7000, 'classifier__solver': 'adam'}\n",
      "0.362 (+/- 0.342) for {'classifier__activation': 'logistic', 'classifier__hidden_layer_sizes': (50, 10), 'classifier__learning_rate': 'constant', 'classifier__learning_rate_init': 0.1, 'classifier__max_iter': 7000, 'classifier__solver': 'adam'}\n",
      "0.166 (+/- 0.002) for {'classifier__activation': 'logistic', 'classifier__hidden_layer_sizes': (50, 10), 'classifier__learning_rate': 'constant', 'classifier__learning_rate_init': 0.2, 'classifier__max_iter': 7000, 'classifier__solver': 'adam'}\n",
      "0.832 (+/- 0.144) for {'classifier__activation': 'logistic', 'classifier__hidden_layer_sizes': (100,), 'classifier__learning_rate': 'constant', 'classifier__learning_rate_init': 0.01, 'classifier__max_iter': 7000, 'classifier__solver': 'adam'}\n",
      "0.146 (+/- 0.027) for {'classifier__activation': 'logistic', 'classifier__hidden_layer_sizes': (100,), 'classifier__learning_rate': 'constant', 'classifier__learning_rate_init': 0.5, 'classifier__max_iter': 7000, 'classifier__solver': 'adam'}\n",
      "0.273 (+/- 0.309) for {'classifier__activation': 'logistic', 'classifier__hidden_layer_sizes': (100,), 'classifier__learning_rate': 'constant', 'classifier__learning_rate_init': 0.1, 'classifier__max_iter': 7000, 'classifier__solver': 'adam'}\n",
      "0.169 (+/- 0.13) for {'classifier__activation': 'logistic', 'classifier__hidden_layer_sizes': (100,), 'classifier__learning_rate': 'constant', 'classifier__learning_rate_init': 0.2, 'classifier__max_iter': 7000, 'classifier__solver': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "# Read the dataset\n",
    "data = pd.read_csv('ObesityDataSet.csv')\n",
    "\n",
    "# Keep only the desired columns\n",
    "desired_columns = ['Height', 'Weight', 'eat_vegetables_frequency', 'exercise_frequency','water_day','Age','device_time', 'Gender',\n",
    "                   'family_history_with_overweight', 'eat_high_caloric_food', 'eat_between_meals', 'smoke', 'monitor_calories', 'transport_mode']\n",
    "data = data[desired_columns + ['obese_category']]  # Add 'obese_category' to keep it as well\n",
    "\n",
    "# Apply label encoding to 'obese_category'\n",
    "label_encoder = LabelEncoder()\n",
    "data['obese_category'] = label_encoder.fit_transform(data['obese_category'])\n",
    "\n",
    "# Extract features and target variable\n",
    "tr_features = data.drop(columns=['obese_category'])  # Features\n",
    "tr_labels = data['obese_category']  # Target variable\n",
    "\n",
    "# Define the columns to one-hot encode\n",
    "nominal_columns = ['Gender', 'family_history_with_overweight', 'eat_high_caloric_food', \n",
    "                   'eat_between_meals', 'smoke', 'monitor_calories', 'transport_mode']\n",
    "\n",
    "# Create a pipeline for preprocessing\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('onehot', OneHotEncoder(), nominal_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Define the MLP model\n",
    "mlp = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', MLPClassifier())\n",
    "])\n",
    "\n",
    "# Define the parameters for grid search\n",
    "parameters = {\n",
    "    'classifier__hidden_layer_sizes': [(10,), (50, 10), (100,)],\n",
    "    'classifier__activation': ['relu', 'tanh', 'logistic'],\n",
    "    'classifier__learning_rate': ['constant'],\n",
    "    'classifier__learning_rate_init': [0.01, 0.5, 0.1, 0.2],\n",
    "    'classifier__max_iter': [7000],\n",
    "    'classifier__solver': ['adam']\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "cv = GridSearchCV(mlp, parameters, cv=5)\n",
    "cv.fit(tr_features, tr_labels)\n",
    "\n",
    "# Print results\n",
    "def print_results(results):\n",
    "    print('BEST PARAMS: {}\\n'.format(results.best_params_))\n",
    "    \n",
    "    mean = results.cv_results_['mean_test_score']\n",
    "    stds = results.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(mean, stds, results.cv_results_['params']):\n",
    "        print('{} (+/- {}) for {}'.format(round(mean, 3), round(std * 2, 3), params))\n",
    "\n",
    "print_results(cv)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
